# LSD-TFFormer: Retinex-Based Transformer for Low-Light Image Enhancement

This repository contains the implementation of **LSD-TFFormer**, a Retinex-inspired Transformer model for **Low-Light Image Enhancement (LLIE)**.

The model decomposes an image into illumination and reflectance, enhances reflectance using window-based self-attention, applies denoising, and reconstructs a clean, bright image.

---

## ğŸš€ Features

- Retinex-based image decomposition
- Window-based Transformer attention (efficient)
- Reflectance enhancement module
- CNN-based denoiser
- Mixed precision (AMP) training
- Gradient clipping for stability
- Resume training from checkpoint
- PSNR & SSIM evaluation
- Separate train / validation / test scripts
- GPU support

---

## ğŸ“‚ Project Structure

```
LSD-TFFormer-LLIE/
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ validate.py
â”œâ”€â”€ test.py
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ llie_dataset.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ blocks.py
â”‚   â””â”€â”€ lsd_tf_former.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ checkpoint.py
â”‚
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ results/
â””â”€â”€ README.md
```

---

## ğŸ§  Model Architecture

The model consists of three main components:

### 1ï¸âƒ£ Illumination Estimator
- Predicts a 1-channel illumination map.
- Uses CNN layers + Sigmoid activation.
- Prevents division instability using clamping.

### 2ï¸âƒ£ Reflectance Restoration (Transformer)
- Extracts features using convolution.
- Uses multiple Transformer blocks.
- Applies window-based attention (8Ã—8).
- Restores enhanced reflectance.

### 3ï¸âƒ£ Denoiser
- CNN residual denoiser.
- Removes noise after enhancement.

### ğŸ” Final Reconstruction

```
Reflectance = Input / Illumination
Enhanced = Transformer(Reflectance)
Denoised = Denoiser(Enhanced)
Output = Denoised Ã— Illumination
```

---

## ğŸ“¦ Installation

Clone the repository:

```bash
git clone https://github.com/yourusername/LSD-TFFormer-LLIE.git
cd LSD-TFFormer-LLIE
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ–¥ï¸ GPU Check

Make sure CUDA is available:

```python
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
```

---

## ğŸ“Š Dataset Structure

```
data/
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ low/
â”‚   â””â”€â”€ high/
â”‚
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ low/
â”‚   â””â”€â”€ high/
â”‚
â””â”€â”€ test/
    â””â”€â”€ low/
```

- `low` = dark images  
- `high` = ground-truth bright images  

Update dataset paths inside `config.py`.

---

## ğŸ‹ï¸ Training

To start training:

```bash
python train.py
```

Features:
- Automatic checkpoint saving
- Resume training if checkpoint exists
- Mixed precision (AMP)
- Gradient clipping

---

## ğŸ“ˆ Validation

To evaluate on validation set:

```bash
python validate.py
```

Outputs:
- PSNR (dB)
- SSIM

---

## ğŸ§ª Testing (Inference Only)

To enhance test images:

```bash
python test.py
```

Enhanced images will be saved in:

```
results/
```

---

## âš™ï¸ Training Configuration

Edit `config.py`:

```python
BATCH_SIZE = 4
LR = 5e-5
EPOCHS = 150
IMG_SIZE = 256
```

---

## ğŸ“Š Evaluation Metrics

### PSNR
Peak Signal-to-Noise Ratio

### SSIM
Structural Similarity Index

Both are implemented manually in `utils/metrics.py`.

---

## ğŸ’¾ Checkpointing

The model automatically saves:

- Model weights
- Optimizer state
- AMP scaler state
- Current epoch

Checkpoint file:
```
checkpoints/lsd_tf_checkpoint.pth
```

Training automatically resumes if checkpoint exists.

---

## ğŸ”¥ Performance Notes

Current setup:
- Loss: L1
- Optimizer: Adam
- Window size: 8
- Transformer blocks: 4

Performance can be improved by:
- Adding perceptual loss (VGG/LPIPS)
- Adding multi-scale training
- Adding illumination smoothness loss
- Using cosine LR scheduler

---

## ğŸ›  Requirements

- Python 3.8+
- PyTorch 2.x
- torchvision
- Pillow
- CUDA (recommended)

---

## ğŸ“Œ Future Improvements

- Multi-scale Transformer
- Noise-aware enhancement
- Perceptual loss integration
- NTIRE competition optimization
- Real-time inference optimization

---

## ğŸ“œ License

This project is open-source and free to use for research and educational purposes.

---

## â­ If This Helps You

Please consider starring the repository.
